{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkmw7GBue0MTjLJ3GyWuXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1pawn0/Transformers-Playground/blob/main/Notebooks/toy_transformer_model_via_pytorch_nn_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEpEYY2KGnQy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import SGD, AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import userdata\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n",
        "torch.manual_seed(166320)\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "dtype = torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")\n",
        "train_corpus = ''.join(ds['train']['text'])\n",
        "val_corpus = ''.join(ds['validation']['text'])\n",
        "test_corpus = ''.join(ds['test']['text'])\n",
        "del ds"
      ],
      "metadata": {
        "id": "-rLa7K9QHGsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "def tokenize_large_text_corpus(text_corpus, tokenizer, max_length=512, stride=128):\n",
        "    encodings = tokenizer(\n",
        "        text_corpus,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True\n",
        "    )\n",
        "\n",
        "    return encodings.input_ids\n",
        "\n",
        "train_input_ids = TensorDataset(tokenize_large_text_corpus(train_corpus, tokenizer))\n",
        "train_input_ids_loader = DataLoader(train_input_ids, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "del train_input_ids\n"
      ],
      "metadata": {
        "id": "WJAySeM2Q-iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToyTransformerModel(nn.Module):\n",
        "    def __init__(self, d_model=128, num_embeddings=30000, max_sequence_len=512, tokenizer: AutoTokenizer = tokenizer):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_embeddings = nn.Embedding(num_embeddings, d_model, 0, dtype=dtype, device=device)\n",
        "        self.position_embeddings = nn.Embedding(max_sequence_len, d_model, 0, dtype=dtype, device=device)\n",
        "        self.W_q = nn.Linear(d_model, d_model, dtype=dtype, device=device)\n",
        "        self.W_k = nn.Linear(d_model, d_model, dtype=dtype, device=device)\n",
        "        self.W_v = nn.Linear(d_model, d_model, dtype=dtype, device=device)\n",
        "        self.W_out = nn.Linear(d_model, d_model, dtype=dtype, device=device)\n",
        "        self.ff1 = nn.Linear(d_model, 4 * d_model, dtype=dtype, device=device)\n",
        "        self.ff2 = nn.Linear(4 * d_model, d_model, dtype=dtype, device=device)\n",
        "        self.ln1 = nn.LayerNorm(d_model, dtype=dtype, device=device)\n",
        "        self.ln2 = nn.LayerNorm(d_model, dtype=dtype, device=device)\n",
        "        self.lm_head = nn.Linear(d_model, num_embeddings, bias=False, dtype=dtype, device=device)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        token_emb = self.token_embeddings(input_ids)\n",
        "        positions = torch.arange(seq_len, device=device)\n",
        "        pos_emb = self.position_embeddings(positions).unsqueeze(0).expand(batch_size, seq_len, -1)\n",
        "        x = token_emb + pos_emb\n",
        "\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
        "        attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
        "        attn_weights = self.softmax(attn_scores)\n",
        "        attn_output = torch.matmul(attn_weights, v)\n",
        "        attn_output = self.W_out(attn_output)\n",
        "\n",
        "        x = self.ln1(x + attn_output)\n",
        "\n",
        "        ff_output = self.ff2(self.gelu(self.ff1(x)))\n",
        "\n",
        "        x = self.ln2(x + ff_output)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "model = ToyTransformerModel(\n",
        "    d_model=128,\n",
        "    num_embeddings=tokenizer.vocab_size,\n",
        "    max_sequence_len=512,\n",
        "    tokenizer=tokenizer,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "ESDbaBu7dvwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.float32\n",
        "\n",
        "model = ToyTransformerModel(\n",
        "    d_model=128,\n",
        "    num_embeddings=tokenizer.vocab_size,\n",
        "    max_sequence_len=512,\n",
        "    tokenizer=tokenizer\n",
        ").to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.01\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_input_ids_loader))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(train_input_ids_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch[0].to(device)\n",
        "\n",
        "        inputs = input_ids[:, :-1]\n",
        "        targets = input_ids[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(inputs)\n",
        "\n",
        "        loss = loss_fn(\n",
        "            logits.reshape(-1, model.num_embeddings),\n",
        "            targets.reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'avg_loss': f'{total_loss/num_batches:.4f}'\n",
        "        })\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }\n",
        "        torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "        print(f\"Checkpoint saved: checkpoint_epoch_{epoch+1}.pt\")\n",
        "\n",
        "torch.save(model.state_dict(), 'toy_transformer_final.pt')\n",
        "print(\"\\nTraining complete! Final model saved as 'toy_transformer_final.pt'\")"
      ],
      "metadata": {
        "id": "Lj_ygF0rtoiQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
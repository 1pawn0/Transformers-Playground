{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN18Wg7OmYeAPpB7HS8PajG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1pawn0/Transformers-Playground/blob/main/Notebooks/attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "N2_VVu4f_WYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(83160)\n",
        "vocab_size = 512\n",
        "embedding_dim = 81  # dimensionality of model (d_model)\n",
        "num_heads = 3  # number of attention heads\n",
        "d_k = int(embedding_dim / num_heads)  # dimensionality of the key(and query) vectors, d_key == d_query\n",
        "embedding_matrix = torch.randn(vocab_size, embedding_dim, requires_grad=True)  # the lookup table of tokens\n",
        "input_seq_tokens_vec = torch.tensor(\n",
        "    [47, 53, 29, 23, 17, 19, 31]\n",
        ")  # input vector (e.g. one sentence in natural language)\n",
        "X = embedding_matrix[input_seq_tokens_vec]  # input matrix shaped (input_seq_len, d_model)\n",
        "\n",
        "W_Q, W_K, W_V = (\n",
        "    torch.randn(embedding_dim, embedding_dim, requires_grad=True),\n",
        "    torch.randn(embedding_dim, embedding_dim, requires_grad=True),\n",
        "    torch.randn(embedding_dim, embedding_dim, requires_grad=True),\n",
        ")\n",
        "b_Q, b_K, b_V = (\n",
        "    torch.randn(embedding_dim, requires_grad=True),\n",
        "    torch.randn(embedding_dim, requires_grad=True),\n",
        "    torch.randn(embedding_dim, requires_grad=True),\n",
        ")\n",
        "\n",
        "Q = X @ W_Q + b_Q\n",
        "K = X @ W_K + b_K\n",
        "V = X @ W_V + b_V\n",
        "\n",
        "print(\"Single-head shapes: \", Q.shape, K.shape, V.shape)\n",
        "\n",
        "\n",
        "# Reshape Q,K,V Tensors for Multi-head attention\n",
        "input_seq_len = X.shape[0]\n",
        "Q = Q.view(input_seq_len, num_heads, d_k).transpose(\n",
        "    0, 1\n",
        ")  # shape: (num_heads, input_seq_len, d_k= embedding_dim/num_heads)\n",
        "K = K.view(input_seq_len, num_heads, d_k).transpose(\n",
        "    0, 1\n",
        ")  # shape: (num_heads, input_seq_len, d_k= embedding_dim/num_heads)\n",
        "V = V.view(input_seq_len, num_heads, d_k).transpose(\n",
        "    0, 1\n",
        ")  # shape: (num_heads, input_seq_len, d_k= embedding_dim/num_heads)\n",
        "print(f\"{num_heads}-head shapes: \", Q.shape, K.shape, V.shape)\n",
        "# now `Q,K,V` became 3D tensors with shape: (num_heads, input_seq_len, embedding_dim/num_heads)\n",
        "\n",
        "\n",
        "scores = Q @ K.transpose(-1, -2) / (d_k**0.5)\n",
        "weights = torch.softmax(scores, dim=-1)\n",
        "attention_output = weights @ V\n",
        "\n",
        "print(attention_output.shape)\n"
      ],
      "metadata": {
        "id": "U5Pzme9JApv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###### `query` tensor shape:\n",
        "\n",
        "**`(batch_size,..., number_of_heads_of_query, target_sequence_length, embedding_dimension_of_the_query_and_key)`**\n",
        "\n",
        "###### `key` tensor shape:\n",
        "\n",
        "**`(batch_size,..., number_of_heads_of_key_and_value, source_sequence_length, embedding_dimension_of_the_query_and_key)`**\n",
        "\n",
        "###### `value` tensor shape:\n",
        "\n",
        "**`(batch_size,..., number_of_heads_of_value, source_sequence_length, embedding_dimension_of_the_value)`**\n",
        "\n",
        "###### `Attention` output tensor shape:\n",
        "\n",
        "**`(batch_size,..., number_of_heads_of_query, target_sequence_length, embedding_dimension_of_the_value)`**\n"
      ],
      "metadata": {
        "id": "T_pKusysiw2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title another approach\n",
        "torch.manual_seed(83160)\n",
        "vocab_size = 512\n",
        "embedding_dim = 81  # dimensionality of model (d_model)\n",
        "num_heads = 3  # number of attention heads\n",
        "d_k = int(embedding_dim / num_heads)  # dimensionality of the key(and query) vectors, d_key == d_query\n",
        "embedding_matrix = torch.randn(vocab_size, embedding_dim, requires_grad=True)  # the lookup table of tokens\n",
        "input_seq_tokens_vec = torch.tensor(\n",
        "    [47, 53, 29, 23, 17, 19, 31]\n",
        ")  # input vector (e.g. one sentence in natural language)\n",
        "X = embedding_matrix[input_seq_tokens_vec]  # input matrix shaped (input_seq_len, d_model)\n",
        "\n",
        "# 3 separate (embedding_dim, d_k) matrices for each head\n",
        "W_Q = torch.randn(num_heads, embedding_dim, d_k, requires_grad=True)\n",
        "W_K = torch.randn(num_heads, embedding_dim, d_k, requires_grad=True)\n",
        "W_V = torch.randn(num_heads, embedding_dim, d_k, requires_grad=True)\n",
        "\n",
        "b_Q = torch.randn(num_heads, d_k, requires_grad=True)\n",
        "b_K = torch.randn(num_heads, d_k, requires_grad=True)\n",
        "b_V = torch.randn(num_heads, d_k, requires_grad=True)\n",
        "\n",
        "\n",
        "Q_heads, K_heads, V_heads = [], [], []\n",
        "\n",
        "for h in range(num_heads):\n",
        "    Q_heads.append(X @ W_Q[h] + b_Q[h])\n",
        "    K_heads.append(X @ W_K[h] + b_K[h])\n",
        "    V_heads.append(X @ W_V[h] + b_V[h])\n",
        "\n",
        "Q = torch.stack(Q_heads, dim=0)\n",
        "K = torch.stack(K_heads, dim=0)\n",
        "V = torch.stack(V_heads, dim=0)\n",
        "\n",
        "print(f\"{num_heads}-head shapes: \", Q.shape, K.shape, V.shape)\n",
        "\n",
        "scores = Q @ K.transpose(-1, -2) / (d_k**0.5)\n",
        "weights = torch.softmax(scores, dim=-1)\n",
        "attention_output = weights @ V\n",
        "\n",
        "print(attention_output.shape)\n"
      ],
      "metadata": {
        "id": "ntLVF3JIOrQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}